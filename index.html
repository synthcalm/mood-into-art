<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Mood Into Art - Voice Debug</title>
</head>
<body>
  <div>
    <h2>Transcription Debug</h2>
    <button id="startVoice">Start Voice</button>
    <div id="transcribingIndicator" style="display:none;">🎙 Listening...</div>
    <textarea id="activityInput" rows="4" placeholder="Describe your mood..."></textarea>
  </div>

  <script>
    const activityInput = document.getElementById('activityInput');
    const transcribingIndicator = document.getElementById('transcribingIndicator');
    const startVoiceButton = document.getElementById('startVoice');

    let socket, audioContext, mediaStream, workletNode;
    let isRecording = false;

    async function unlockAudioContextAndMic() {
      try {
        audioContext = new (window.AudioContext || window.webkitAudioContext)();
        if (audioContext.state === 'suspended') {
          await audioContext.resume();
        }
        mediaStream = await navigator.mediaDevices.getUserMedia({ audio: true });
        return true;
      } catch (e) {
        alert("❌ Microphone access failed: " + e.message);
        return false;
      }
    }

    async function startRecording() {
      try {
        console.log("🔄 Getting AssemblyAI token...");
        const { token } = await fetch("https://mood-into-art-backend.onrender.com/assemblyai-token")
          .then(res => res.json());

        socket = new WebSocket(`wss://api.assemblyai.com/v2/realtime/ws?sample_rate=16000&token=${token}`);

        socket.onmessage = (msg) => {
          const data = JSON.parse(msg.data);
          console.log("📥 AssemblyAI message:", data);
          if (data.text) {
            console.log("📝 Transcription:", data.text);
            activityInput.value = data.text;
          }
        };

        socket.onopen = async () => {
          console.log("✅ WebSocket connected");
          transcribingIndicator.style.display = 'block';

          await audioContext.audioWorklet.addModule(URL.createObjectURL(new Blob([`
            class AudioProcessor extends AudioWorkletProcessor {
              constructor() {
                super();
              }
              process(inputs) {
                const input = inputs[0][0];
                if (!input) return true;

                const downsampled = downsampleBuffer(input, sampleRate, 16000);
                const int16 = floatTo16BitPCM(downsampled);
                this.port.postMessage(int16);
                return true;
              }
            }

            function floatTo16BitPCM(input) {
              const output = new Int16Array(input.length);
              for (let i = 0; i < input.length; i++) {
                const s = Math.max(-1, Math.min(1, input[i]));
                output[i] = s * 0x7FFF;
              }
              return output.buffer;
            }

            function downsampleBuffer(buffer, sampleRate, outRate) {
              if (outRate === sampleRate) return buffer;
              const ratio = sampleRate / outRate;
              const newLength = Math.round(buffer.length / ratio);
              const result = new Float32Array(newLength);
              let offset = 0;
              for (let i = 0; i < newLength; i++) {
                const nextOffset = Math.round((i + 1) * ratio);
                let sum = 0, count = 0;
                for (let j = offset; j < nextOffset && j < buffer.length; j++) {
                  sum += buffer[j]; count++;
                }
                result[i] = sum / count;
                offset = nextOffset;
              }
              return result;
            }

            registerProcessor('audio-processor', AudioProcessor);
          `], { type: "application/javascript" })));

          const source = audioContext.createMediaStreamSource(mediaStream);
          workletNode = new AudioWorkletNode(audioContext, 'audio-processor');

          workletNode.port.onmessage = (e) => {
            if (socket.readyState === WebSocket.OPEN) {
              socket.send(e.data);  // ✅ Send raw PCM buffer
              console.log("📤 Sent audio chunk");
            }
          };

          source.connect(workletNode);
          workletNode.connect(audioContext.destination);
          console.log("🔗 Audio pipeline connected");
        };

        socket.onclose = () => {
          console.warn("🛑 WebSocket closed");
          stopRecording();
        };

        socket.onerror = (err) => {
          console.error("❌ WebSocket error:", err);
        };

      } catch (e) {
        alert("⚠️ Failed to start recording: " + e.message);
        stopRecording();
      }
    }

    function stopRecording() {
      if (socket && socket.readyState === WebSocket.OPEN) socket.close();
      if (audioContext && audioContext.state !== 'closed') audioContext.close();
      if (mediaStream) mediaStream.getTracks().forEach(track => track.stop());
      transcribingIndicator.style.display = 'none';
      console.log("🛑 Recording stopped");
    }

    startVoiceButton.addEventListener("click", async () => {
      if (!isRecording) {
        isRecording = true;
        startVoiceButton.textContent = "Stop Voice";

        const micReady = await unlockAudioContextAndMic();
        if (micReady) {
          await startRecording();
        } else {
          isRecording = false;
          startVoiceButton.textContent = "Start Voice";
        }

      } else {
        isRecording = false;
        startVoiceButton.textContent = "Start Voice";
        stopRecording();
      }
    });
  </script>
</body>
</html>
