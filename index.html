<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>MIA - Safari Safe AudioWorklet</title>
</head>
<body>
  <div>
    <h2>Voice Capture - AudioWorklet (Safari Fix)</h2>
    <button id="startVoice">Start Voice</button>
    <div id="transcribingIndicator" style="display:none;">Listening...</div>
    <textarea id="activityInput" rows="4" placeholder="Describe your mood..."></textarea>
  </div>

  <script>
    const activityInput = document.getElementById('activityInput');
    const transcribingIndicator = document.getElementById('transcribingIndicator');
    const startVoiceButton = document.getElementById('startVoice');

    let socket, audioContext, mediaStream, workletNode;
    let isRecording = false;

    async function startRecording() {
      try {
        console.log("üîÑ Requesting AssemblyAI token...");
        const { token } = await fetch("https://mood-into-art-backend.onrender.com/assemblyai-token")
          .then(res => res.json());

        console.log("‚úÖ Token received, connecting WebSocket...");
        socket = new WebSocket(`wss://api.assemblyai.com/v2/realtime/ws?sample_rate=16000&token=${token}`);

        socket.onmessage = (msg) => {
          const data = JSON.parse(msg.data);
          if (data.text) {
            console.log("üìù Transcription:", data.text);
            activityInput.value = data.text;
          }
        };

        socket.onopen = async () => {
          console.log("‚úÖ WebSocket connected");
          transcribingIndicator.style.display = 'block';

          console.log("üéô Requesting microphone access...");
          mediaStream = await navigator.mediaDevices.getUserMedia({ audio: true });

          console.log("üéß Initializing AudioContext with AudioWorklet...");
          audioContext = new (window.AudioContext || window.webkitAudioContext)();
          await audioContext.audioWorklet.addModule(URL.createObjectURL(new Blob([`
            class AudioProcessor extends AudioWorkletProcessor {
              constructor() {
                super();
              }
              process(inputs) {
                const input = inputs[0][0];
                if (input) {
                  const downsampled = downsampleBuffer(input, sampleRate, 16000);
                  const int16 = floatTo16BitPCM(downsampled);
                  this.port.postMessage(int16);
                }
                return true;
              }
            }
            function floatTo16BitPCM(input) {
              const len = input.length;
              const result = new Int16Array(len);
              for (let i = 0; i < len; i++) {
                result[i] = Math.max(-1, Math.min(1, input[i])) * 32767;
              }
              return result.buffer;
            }
            function downsampleBuffer(buffer, sampleRate, outRate) {
              if (outRate === sampleRate) return buffer;
              const sampleRateRatio = sampleRate / outRate;
              const newLength = Math.round(buffer.length / sampleRateRatio);
              const result = new Float32Array(newLength);
              let offset = 0;
              for (let i = 0; i < newLength; i++) {
                const nextOffset = Math.round((i + 1) * sampleRateRatio);
                let sum = 0, count = 0;
                for (let j = offset; j < nextOffset && j < buffer.length; j++) {
                  sum += buffer[j];
                  count++;
                }
                result[i] = sum / count;
                offset = nextOffset;
              }
              return result;
            }
            registerProcessor('audio-processor', AudioProcessor);
          `], { type: "application/javascript" })));

          if (audioContext.state === 'suspended') {
            console.log("üîî Resuming audio context...");
            await audioContext.resume();
          }

          const source = audioContext.createMediaStreamSource(mediaStream);
          workletNode = new AudioWorkletNode(audioContext, 'audio-processor');

          workletNode.port.onmessage = (e) => {
            if (socket.readyState === WebSocket.OPEN) {
              socket.send(e.data);
            }
          };

          source.connect(workletNode).connect(audioContext.destination);
        };

        socket.onclose = () => {
          console.log("üõë WebSocket closed");
          stopRecording();
        };

        socket.onerror = (e) => {
          console.error("‚ùå WebSocket error:", e);
        };

      } catch (e) {
        console.error("‚ùå Error starting recording:", e.message);
        alert("Failed to start recording: " + e.message);
      }
    }

    function stopRecording() {
      try {
        if (socket && socket.readyState === WebSocket.OPEN) socket.close();
        if (audioContext && audioContext.state !== 'closed') audioContext.close();
        if (mediaStream) mediaStream.getTracks().forEach(track => track.stop());
        transcribingIndicator.style.display = 'none';
        console.log("üõë Recording stopped");
      } catch (err) {
        console.warn("‚ö†Ô∏è Error during cleanup:", err.message);
      }
    }

    startVoiceButton.addEventListener("click", async () => {
      if (!isRecording) {
        console.log("‚ñ∂Ô∏è Starting voice capture...");
        isRecording = true;
        startVoiceButton.textContent = "Stop Voice";
        await startRecording();
      } else {
        console.log("‚èπ Stopping voice capture...");
        isRecording = false;
        startVoiceButton.textContent = "Start Voice";
        stopRecording();
      }
    });
  </script>
</body>
</html>
