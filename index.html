<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>MIA - Debug Transcription</title>
</head>
<body>
  <div>
    <h2>Transcription Debug</h2>
    <button id="startVoice">Start Voice</button>
    <div id="transcribingIndicator" style="display:none;">Listening...</div>
    <textarea id="activityInput" rows="4" placeholder="Describe your mood..."></textarea>
  </div>

  <script>
    const activityInput = document.getElementById('activityInput');
    const transcribingIndicator = document.getElementById('transcribingIndicator');
    const startVoiceButton = document.getElementById('startVoice');

    let socket, audioContext, mediaStream, workletNode;
    let isRecording = false;

    async function unlockAudioContextAndMic() {
      try {
        audioContext = new (window.AudioContext || window.webkitAudioContext)({ sampleRate: 16000 });
        if (audioContext.state === 'suspended') {
          await audioContext.resume();
        }
        mediaStream = await navigator.mediaDevices.getUserMedia({ audio: true });
        return true;
      } catch (e) {
        console.error("Microphone access failed:", e);
        alert("Microphone access failed: " + e.message);
        return false;
      }
    }

    async function startRecording() {
      try {
        const response = await fetch("https://mood-into-art-backend.onrender.com/assemblyai-token");
        if (!response.ok) throw new Error(`Token fetch failed: ${response.statusText}`);
        const { token } = await response.json();
        if (!token || typeof token !== 'string') throw new Error("Invalid token received");

        socket = new WebSocket(`wss://api.assemblyai.com/v2/realtime/ws?sample_rate=16000&token=${token}`);

        socket.onopen = async () => {
          console.log("âœ… WebSocket connection opened");
          transcribingIndicator.style.display = 'block';

          if (audioContext.state === 'closed') {
            console.error("AudioContext already closed, aborting setup");
            stopRecording();
            return;
          }

          await audioContext.audioWorklet.addModule(URL.createObjectURL(new Blob([`
            class AudioProcessor extends AudioWorkletProcessor {
              constructor() {
                super();
              }
              process(inputs) {
                const input = inputs[0]?.[0];
                if (input && input.length > 0) {
                  const downsampled = downsampleBuffer(input, sampleRate, 16000);
                  const int16 = floatTo16BitPCM(downsampled);
                  this.port.postMessage(int16);
                }
                return true;
              }
            }
            function floatTo16BitPCM(input) {
              const len = input.length;
              const result = new Int16Array(len);
              for (let i = 0; i < len; i++) {
                result[i] = Math.max(-1, Math.min(1, input[i])) * 32767;
              }
              return result.buffer;
            }
            function downsampleBuffer(buffer, sampleRate, outRate) {
              if (outRate === sampleRate) return buffer;
              const sampleRateRatio = sampleRate / outRate;
              const newLength = Math.round(buffer.length / sampleRateRatio);
              const result = new Float32Array(newLength);
              let offset = 0;
              for (let i = 0; i < newLength; i++) {
                const nextOffset = Math.round((i + 1) * sampleRateRatio);
                let sum = 0, count = 0;
                for (let j = offset; j < nextOffset && j < buffer.length; j++) {
                  sum += buffer[j];
                  count++;
                }
                result[i] = count > 0 ? sum / count : 0;
                offset = nextOffset;
              }
              return result;
            }
            registerProcessor('audio-processor', AudioProcessor);
          `], { type: "application/javascript" })));

          const source = audioContext.createMediaStreamSource(mediaStream);
          workletNode = new AudioWorkletNode(audioContext, 'audio-processor');

          workletNode.port.onmessage = (e) => {
            if (socket.readyState === WebSocket.OPEN) {
              const audioData = e.data;
              const base64Audio = btoa(String.fromCharCode(...new Uint8Array(audioData)));
              socket.send(JSON.stringify({ audio_data: base64Audio }));
              console.log("ðŸ“¤ Sent audio chunk (base64):", base64Audio.slice(0, 50) + "...");
            }
          };

          source.connect(workletNode);
          workletNode.connect(audioContext.destination);
        };

        socket.onmessage = (msg) => {
          const data = JSON.parse(msg.data);
          console.log("ðŸ“¥ AssemblyAI message:", data);
          if (data.error) {
            console.error("AssemblyAI error:", data.error);
            alert("AssemblyAI error: " + data.error);
            stopRecording();
          } else if (data.message_type === "FinalTranscript" && data.text) {
            console.log("ðŸ“ Final transcription:", data.text);
            activityInput.value = data.text;
          } else if (data.message_type === "PartialTranscript" && data.text) {
            console.log("ðŸ“ Partial transcription:", data.text);
            activityInput.value = data.text; // Optional: show partial results
          }
        };

        socket.onclose = () => {
          console.warn("ðŸ›‘ WebSocket closed");
          stopRecording();
        };

        socket.onerror = (e) => {
          console.error("âŒ WebSocket error:", e);
        };

      } catch (e) {
        console.error("Failed to start recording:", e);
        alert("Failed to start recording: " + e.message);
        stopRecording();
      }
    }

    function stopRecording() {
      try {
        if (socket && socket.readyState !== WebSocket.CLOSED) socket.close();
        if (audioContext && audioContext.state !== 'closed') audioContext.close();
        if (mediaStream) mediaStream.getTracks().forEach(track => track.stop());
        if (workletNode) workletNode.disconnect();
        transcribingIndicator.style.display = 'none';
        isRecording = false;
        startVoiceButton.textContent = "Start Voice";
        console.log("ðŸ›‘ Recording stopped");
      } catch (err) {
        console.warn("âš ï¸ Error during cleanup:", err.message);
      }
    }

    startVoiceButton.addEventListener("click", async () => {
      if (!isRecording) {
        isRecording = true;
        startVoiceButton.textContent = "Stop Voice";
        const unlocked = await unlockAudioContextAndMic();
        if (unlocked) {
          await startRecording();
        } else {
          isRecording = false;
          startVoiceButton.textContent = "Start Voice";
        }
      } else {
        stopRecording();
      }
    });
  </script>
</body>
</html>
