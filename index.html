<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>MIA - Debug Transcription</title>
</head>
<body>
  <div>
    <h2>Transcription Debug</h2>
    <button id="startVoice">Start Voice</button>
    <div id="transcribingIndicator" style="display:none;">Listening...</div>
    <textarea id="activityInput" rows="4" placeholder="Describe your mood..."></textarea>
  </div>

  <script>
    const activityInput = document.getElementById('activityInput');
    const transcribingIndicator = document.getElementById('transcribingIndicator');
    const startVoiceButton = document.getElementById('startVoice');

    let socket, audioContext, mediaStream, workletNode;
    let isRecording = false;

    async function unlockAudioContextAndMic() {
      try {
        audioContext = new (window.AudioContext || window.webkitAudioContext)();
        if (audioContext.state === 'suspended') {
          await audioContext.resume();
        }

        mediaStream = await navigator.mediaDevices.getUserMedia({ audio: true });
        return true;
      } catch (e) {
        alert("Microphone access failed: " + e.message);
        return false;
      }
    }

    async function startRecording() {
      try {
        const { token } = await fetch("https://mood-into-art-backend.onrender.com/assemblyai-token")
          .then(res => res.json());

        socket = new WebSocket(`wss://api.assemblyai.com/v2/realtime/ws?sample_rate=16000&token=${token}`);

        socket.onmessage = (msg) => {
          const data = JSON.parse(msg.data);
          console.log("ðŸ“¥ AssemblyAI message:", data);

          if (data.text) {
            console.log("ðŸ“ Transcription received:", data.text);
            activityInput.value = data.text;
          }
        };

        socket.onopen = async () => {
          transcribingIndicator.style.display = 'block';

          // Add the audio processor code into the browser
          await audioContext.audioWorklet.addModule(URL.createObjectURL(new Blob([`
            class AudioProcessor extends AudioWorkletProcessor {
              constructor() {
                super();
              }
              process(inputs) {
                const input = inputs[0]?.[0];
                if (input && input.length > 0) {
                  const downsampled = downsampleBuffer(input, sampleRate, 16000);
                  const int16 = floatTo16BitPCM(downsampled);
                  this.port.postMessage(int16);
                }
                return true;
              }
            }

            function floatTo16BitPCM(input) {
              const len = input.length;
              const result = new Int16Array(len);
              for (let i = 0; i < len; i++) {
                result[i] = Math.max(-1, Math.min(1, input[i])) * 32767;
              }
              return result.buffer;
            }

            function downsampleBuffer(buffer, sampleRate, outRate) {
              if (outRate === sampleRate) return buffer;
              const sampleRateRatio = sampleRate / outRate;
              const newLength = Math.round(buffer.length / sampleRateRatio);
              const result = new Float32Array(newLength);
              let offset = 0;
              for (let i = 0; i < newLength; i++) {
                const nextOffset = Math.round((i + 1) * sampleRateRatio);
                let sum = 0, count = 0;
                for (let j = offset; j < nextOffset && j < buffer.length; j++) {
                  sum += buffer[j];
                  count++;
                }
                result[i] = sum / count;
                offset = nextOffset;
              }
              return result;
            }

            registerProcessor('audio-processor', AudioProcessor);
          `], { type: "application/javascript" })));

          const source = audioContext.createMediaStreamSource(mediaStream);
          workletNode = new AudioWorkletNode(audioContext, 'audio-processor');

          // ðŸ”¥ Send raw audio directly (no base64!)
          workletNode.port.onmessage = (e) => {
            if (socket.readyState === WebSocket.OPEN) {
              socket.send(e.data);  // e.data is raw binary buffer
              console.log("ðŸ“¤ Sent audio chunk (raw)");
            }
          };

          source.connect(workletNode);
          workletNode.connect(audioContext.destination);
        };

        socket.onclose = () => {
          console.warn("ðŸ›‘ WebSocket closed");
          stopRecording();
        };

        socket.onerror = (e) => {
          console.error("âŒ WebSocket error:", e);
        };

      } catch (e) {
        alert("Failed to start recording: " + e.message);
      }
    }

    function stopRecording() {
      try {
        if (socket && socket.readyState === WebSocket.OPEN) socket.close();
        if (audioContext && audioContext.state !== 'closed') audioContext.close();
        if (mediaStream) mediaStream.getTracks().forEach(track => track.stop());
        transcribingIndicator.style.display = 'none';
        console.log("ðŸ›‘ Recording stopped");
      } catch (err) {
        console.warn("âš ï¸ Error during cleanup:", err.message);
      }
    }

    startVoiceButton.addEventListener("click", async () => {
      if (!isRecording) {
        isRecording = true;
        startVoiceButton.textContent = "Stop Voice";

        const unlocked = await unlockAudioContextAndMic();
        if (unlocked) {
          await startRecording();
        } else {
          isRecording = false;
          startVoiceButton.textContent = "Start Voice";
        }

      } else {
        isRecording = false;
        startVoiceButton.textContent = "Start Voice";
        stopRecording();
      }
    });
  </script>
</body>
</html>
