<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>MIA Debug Mode</title>
</head>
<body>
  <div>
    <h2>Debug Version - Voice Input Test</h2>
    <button id="startVoice">Start Voice</button>
    <div id="transcribingIndicator" style="display:none;">Listening...</div>
    <textarea id="activityInput" rows="4" placeholder="Describe your mood..."></textarea>
  </div>

  <script>
    const ASSEMBLYAI_API_KEY = 'eeee5d1982444610a670bd17152a8e4a';
    const activityInput = document.getElementById('activityInput');
    const transcribingIndicator = document.getElementById('transcribingIndicator');
    const startVoiceButton = document.getElementById('startVoice');

    let socket, audioContext, mediaStream, processorNode, sourceNode;
    let isRecording = false;

    async function startRecording() {
      try {
        console.log("üîÑ Requesting AssemblyAI token...");
        const { token } = await fetch("https://mood-into-art-backend.onrender.com/assemblyai-token")
          .then(res => res.json());


        console.log("‚úÖ Token received, connecting WebSocket...");
        socket = new WebSocket(`wss://api.assemblyai.com/v2/realtime/ws?sample_rate=16000&token=${token}`);

        socket.onmessage = (msg) => {
          const data = JSON.parse(msg.data);
          if (data.text) {
            console.log("üìù Transcription:", data.text);
            activityInput.value = data.text;
          }
        };

        socket.onopen = async () => {
          console.log("‚úÖ WebSocket connected");
          transcribingIndicator.style.display = 'block';
          console.log("üéô Requesting microphone access...");
          mediaStream = await navigator.mediaDevices.getUserMedia({ audio: true });

          console.log("üéß Initializing AudioContext...");
          audioContext = new (window.AudioContext || window.webkitAudioContext)({ sampleRate: 16000 });
          sourceNode = audioContext.createMediaStreamSource(mediaStream);
          processorNode = audioContext.createScriptProcessor(4096, 1, 1);

          processorNode.onaudioprocess = (e) => {
            const input = e.inputBuffer.getChannelData(0);
            const int16 = floatTo16BitPCM(input);
            if (socket.readyState === WebSocket.OPEN) socket.send(int16);
          };

          sourceNode.connect(processorNode);
          processorNode.connect(audioContext.destination);
        };

        socket.onerror = (e) => console.error("‚ùå WebSocket error:", e);
        socket.onclose = () => console.log("üõë WebSocket closed");

      } catch (e) {
        console.error("‚ùå Error starting recording:", e.message);
        alert("Failed to start recording: " + e.message);
      }
    }

    function stopRecording() {
      if (socket) socket.close();
      if (audioContext) audioContext.close();
      if (mediaStream) mediaStream.getTracks().forEach(track => track.stop());
      transcribingIndicator.style.display = 'none';
      console.log("üõë Recording stopped");
    }

    function floatTo16BitPCM(input) {
      const len = input.length;
      const result = new Int16Array(len);
      for (let i = 0; i < len; i++) {
        result[i] = Math.max(-1, Math.min(1, input[i])) * 32767;
      }
      return result.buffer;
    }

    startVoiceButton.addEventListener("click", async () => {
      if (!isRecording) {
        console.log("‚ñ∂Ô∏è Starting voice capture...");
        isRecording = true;
        startVoiceButton.textContent = "Stop Voice";
        await startRecording();
      } else {
        console.log("‚èπ Stopping voice capture...");
        isRecording = false;
        startVoiceButton.textContent = "Start Voice";
        stopRecording();
      }
    });
  </script>
</body>
</html>
